# -*- coding: utf-8 -*-
"""Instruction_Dataset_Synth_bonito_Dark_Matter_Review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FwCfZtLoFuaMDSiksOON-W0FHVIPSKz-

The aim of this notebook is to do initial explorations in generating an Instruction tuning dataset.

References:
* https://towardsdatascience.com/how-to-generate-instruction-datasets-from-any-documents-for-llm-fine-tuning-abb319a05d91
* https://colab.research.google.com/drive/1XuDRVKpUUqdjrqg2-P2FIqkdAQBnqoNL?usp=sharing

The paper being used is a Dark matter review paper: https://arxiv.org/pdf/2104.11488.pdf
"""

!pip install -e git+https://github.com/BatsResearch/bonito#egg=bonito
!pip install datasets huggingface_hub
!pip install pymupdf spacy

!git clone https://github.com/BatsResearch/bonito.git
!pip install -U bonito/

import fitz  # PyMuPDF

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

pdf_path = '/content/dm_review.pdf'
text = extract_text_from_pdf(pdf_path)

text[:1000]

# Split text into sentences

import spacy

nlp = spacy.load("en_core_web_sm")  # Load English tokenizer, tagger, parser, NER, and word vectors

def split_into_sentences(text):
    doc = nlp(text)
    sentences = [sent.text.strip() for sent in doc.sents]
    return sentences

sentences = split_into_sentences(text)

len(sentences)

sentences[200]

# Create a Transformers dataset

from datasets import Dataset

# Assuming sentences is a list of strings, where each string is a sentence
data = {"sentence": sentences}
dataset = Dataset.from_dict(data)

print(dataset)

Dataset.from_dict(data).to_pandas() # the split sentences from the paper

# Generate synthetic dataset using Bonito

from bonito import Bonito
from vllm import SamplingParams

from datasets import load_dataset

# Initialize the Bonito model
bonito = Bonito("BatsResearch/bonito-v1")

# load dataset with unannotated text

# Supported Task Types [full name (short form)]: extractive question answering (exqa),
# multiple-choice question answering (mcqa),
# question generation (qg),
# question answering without choices (qa),
# yes-no question answering (ynqa),
# coreference resolution (coref),
# paraphrase generation (paraphrase),
# paraphrase identification (paraphrase_id),
# sentence completion (sent_comp),
# sentiment (sentiment),
# summarization (summarization),
# text generation (text_gen),
# topic classification (topic_class),
# word sense disambiguation (wsd), textual entailment (te), natural language inference (nli)
# Generate synthetic instruction tuning dataset

# Trying qa: Question answering without choices for now

sampling_params = SamplingParams(max_tokens=256, top_p=0.95, temperature=0.5, n=1)

synthetic_dataset = bonito.generate_tasks(
    dataset,
    context_col="sentence",
    task_type="qa",
    sampling_params=sampling_params
)

print(synthetic_dataset)

from pprint import pprint

pprint("----Generated Instructions----")
pprint(f'Input: {synthetic_dataset[0]["input"]}')
pprint(f'Output: {synthetic_dataset[0]["output"]}')

pprint("----Generated Instructions----")
pprint(f'Input: {synthetic_dataset[100]["input"]}')
pprint(f'Output: {synthetic_dataset[100]["output"]}')

pprint("----Generated Instructions----")
pprint(f'Input: {synthetic_dataset[140]["input"]}')
pprint(f'Output: {synthetic_dataset[140]["output"]}')

pprint("----Generated Instructions----")
pprint(f'Input: {synthetic_dataset[200]["input"]}')
pprint(f'Output: {synthetic_dataset[200]["output"]}')

import pandas as pd

df = pd.DataFrame(synthetic_dataset)

df.iloc[8]['input'], df.iloc[8]['output']

from huggingface_hub import notebook_login

notebook_login()

from huggingface_hub import create_repo
from huggingface_hub import Repository

repo_name = "dark_matter_instruction_qa"  # Choose a name for your dataset repository
repo_url = create_repo(repo_name, repo_type="dataset")
print("Repository URL:", repo_url)

synthetic_dataset.push_to_hub(f"delayedkarma/dark_matter_instruction_qa")

"""Let's try some variations on the sampling params"""

from transformers import set_seed
set_seed(42)

sampling_params = SamplingParams(max_tokens=256, top_p=0.95, temperature=0.5, n=1)

synthetic_dataset = bonito.generate_tasks(
    dataset,
    context_col="sentence",
    task_type="qa",
    sampling_params=sampling_params
)

synthetic_dataset.push_to_hub(f"delayedkarma/dark_matter_instruction_qa") # After setting seed

